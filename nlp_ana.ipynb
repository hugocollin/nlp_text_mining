{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Application\\Anaconda\\envs\\Nlp_projet\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from db.models import get_restaurants_with_reviews_and_users\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import pipeline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class NLPAnalysis:\n",
    "    def __init__(self, db_path='sqlite:///restaurant_reviews.db'):\n",
    "        # Initialisation de la connexion à la base de données\n",
    "        self.engine = create_engine(db_path)\n",
    "        self.session = sessionmaker(bind=self.engine)()\n",
    "        self.data = pd.DataFrame()\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "#1\n",
    "    def load_data(self):\n",
    "        # Extraction et transformation des données\n",
    "        raw_data = get_restaurants_with_reviews_and_users(self.session)\n",
    "        data = []\n",
    "        for restaurant in raw_data:\n",
    "            for review in restaurant['reviews']:\n",
    "                data.append({\n",
    "                    'restaurant': restaurant['restaurant'],\n",
    "                    'restaurant_address': restaurant['restaurant_address'],\n",
    "                    'title': review['title'],\n",
    "                    'user_profile': review['user_profile'],\n",
    "                    'date_review': review['date_review'],\n",
    "                    'rating': review['rating'],\n",
    "                    'type_visit': review['type_visit'],\n",
    "                    'num_contributions': review['num_contributions'],\n",
    "                    'review': review['review'],\n",
    "                    'review_cleaned': review['review_cleaned']\n",
    "                })\n",
    "        self.data = pd.DataFrame(data)\n",
    "        self.data['sentiment'] = self.data['rating'].apply(self._sentiment_class)\n",
    "#2\n",
    "    @staticmethod\n",
    "    def _sentiment_class(rating):\n",
    "        return 2 if rating == 3 else (1 if rating >= 4 else 0)\n",
    "#3\n",
    "    def preprocess_reviews(self):\n",
    "        # Tokenisation et padding\n",
    "        self.tokenizer = Tokenizer(num_words=10000)\n",
    "        self.tokenizer.fit_on_texts(self.data['review_cleaned'])\n",
    "        sequences = self.tokenizer.texts_to_sequences(self.data['review_cleaned'])\n",
    "        return pad_sequences(sequences, padding='post', maxlen=200)\n",
    "#4\n",
    "    def train_lstm_model(self):\n",
    "        # Préparation des données\n",
    "        X_pad = self.preprocess_reviews()\n",
    "        y = self.data['sentiment']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Définition et entraînement du modèle LSTM\n",
    "        self.model = Sequential([\n",
    "            Embedding(input_dim=10000, output_dim=128, input_length=200),\n",
    "            LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "        # Évaluation du modèle\n",
    "        y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Précision du modèle LSTM : {accuracy * 100:.2f}%\")\n",
    "#4\n",
    "    def summarize_restaurant_reviews(self, restaurant_name, taille_segment=100, max_length=15, min_length=5):\n",
    "        def segmenter_avis(texte):\n",
    "            mots = texte.split()\n",
    "            for i in range(0, len(mots), taille_segment):\n",
    "                yield ' '.join(mots[i:i + taille_segment])\n",
    "\n",
    "        # Filtrage des données pour le restaurant spécifié\n",
    "        restaurant_data = self.data[self.data['restaurant'] == restaurant_name]\n",
    "        \n",
    "        if restaurant_data.empty:\n",
    "            return pd.DataFrame({\"restaurant\": [restaurant_name], \"resume_avis\": [\"Aucun avis trouvé pour ce restaurant\"]})\n",
    "\n",
    "        # Récupération et concaténation des avis pour le restaurant spécifié\n",
    "        avis = ' '.join(restaurant_data['review_cleaned'])\n",
    "        segments = list(segmenter_avis(avis))\n",
    "        resumes = []\n",
    "\n",
    "        for segment in segments:\n",
    "            try:\n",
    "                resume = self.summarizer(segment, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "                resumes.append(resume)\n",
    "            except Exception as e:\n",
    "                resumes.append(f\"Erreur lors du résumé : {str(e)}\")\n",
    "\n",
    "        # Combinaison des résumés partiels en un résumé global limité à 15 mots\n",
    "        resume_global = ' '.join(resumes)\n",
    "        resume_limit = ' '.join(resume_global.split()[:15])\n",
    "\n",
    "        # Retour sous forme de DataFrame\n",
    "        return pd.DataFrame({\"restaurant\": [restaurant_name], \"resume_avis\": [resume_limit]})\n",
    "#5\n",
    "    def generate_wordcloud(self):\n",
    "        tous_avis = ' '.join(self.data['review_cleaned'])\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(tous_avis)\n",
    "        return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review_cleaned'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#test ma class\u001b[39;00m\n\u001b[0;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m NLPAnalysis()\n\u001b[1;32m----> 3\u001b[0m nlp\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[0;32m      4\u001b[0m nlp\u001b[38;5;241m.\u001b[39mpreprocess_reviews()\n",
      "Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mNLPAnalysis.load_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m restaurant \u001b[38;5;129;01min\u001b[39;00m raw_data:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m restaurant[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestaurant\u001b[39m\u001b[38;5;124m'\u001b[39m: restaurant[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestaurant\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestaurant_address\u001b[39m\u001b[38;5;124m'\u001b[39m: restaurant[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestaurant_address\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_profile\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_profile\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_review\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_review\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_visit\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype_visit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_contributions\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_contributions\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 42\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m: review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m         })\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentiment_class)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review_cleaned'"
     ]
    }
   ],
   "source": [
    "#test ma class\n",
    "nlp = NLPAnalysis()\n",
    "nlp.load_data()\n",
    "nlp.preprocess_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.train_lstm_model()\n",
    "nlp.summarize_restaurant_reviews()\n",
    "nlp.generate_wordcloud()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Nlp_projet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
